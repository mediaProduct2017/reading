{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[爬虫与分类引擎](http://naotu.baidu.com/file/75a55773c8b55acc4e14df0abe8dfa4f)\n",
    "\n",
    "[媒体产品后端重要组件](http://naotu.baidu.com/file/ad5991878f1c972ac62ade830003dd53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# searchEngine2.py\n",
    "# searchEngine3.py (with unit tests of 2 blocks)\n",
    "# mediaWeb.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# searchEngine2.py\n",
    "\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "import sys\n",
    "# sys.stdout may be changed. When echo parameter in create_engine is True, \n",
    "# the engine will log ot stdout. At that circumstance, sys module will be used.\n",
    "\n",
    "from sqlalchemy import Column, ForeignKey, Integer, String\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "# ext means extension, declarative 说明\n",
    "from sqlalchemy.orm import relationship, sessionmaker\n",
    "# orm is Object-relational mapping, which links object with database\n",
    "\n",
    "import urllib2\n",
    "# there is even urllib5, but now most people use requests to replace urllib series\n",
    "from BeautifulSoup import *\n",
    "# there is beautifulsoup4, which is updated in 2017\n",
    "from urlparse import urljoin\n",
    "# there is urlparse3 (2016) and urlparse4 (2016)\n",
    "import re\n",
    "# regular expression\n",
    "\n",
    "ignoreWords = set(['the', 'of', 'to', 'and', 'a', 'in', 'is', 'it'])\n",
    "\n",
    "class Crawler:\n",
    "    # 从一小组网页开始进行广度优先搜索，直到某一给定深度，期间为所有网页建立索引\n",
    "    # 一般都是静态网址\n",
    "    def __init__(self, dbname):\n",
    "        # self.engine = create_engine('mysql+mysqlconnector://new_user:new_password@localhost:3306/menagerie', \n",
    "        #                echo=False)\n",
    "        self.engine = create_engine('mysql+mysqlconnector://new_user:new_password@localhost:3306/%s'% dbname, \n",
    "                        echo=False)        \n",
    "        DBSession = sessionmaker(bind=self.engine)\n",
    "        # DBSession is a class?\n",
    "        self.session=DBSession()\n",
    "        # self.connection = self.engine.connect()\n",
    "        self.connection = self.session\n",
    "        # self.session.execute()\n",
    "\n",
    "    # 解构函数\n",
    "    def __del__(self):\n",
    "        # pass\n",
    "        # self.connection.close()\n",
    "        self.session.close() \n",
    "    \n",
    "    # 爬到的数据放入数据库\n",
    "    def dbcommit(self):\n",
    "        # pass\n",
    "        self.session.commit()\n",
    "\n",
    "    def createindextables(self):\n",
    "    # self means it is instance method, not class method\n",
    "        self.table=dict()\n",
    "        Base=declarative_base()\n",
    "# declarative_base will let sqlalchemy know the classes are special sqlalchemy classes\n",
    "# which correspond to tables in our database\n",
    "        class Urllist(Base):\n",
    "            __tablename__='urllist'\n",
    "            \n",
    "            id=Column(Integer, primary_key=True, autoincrement=True)\n",
    "            # autoincrement=True by default\n",
    "            \n",
    "            url=Column(String(80), nullable=False)\n",
    "            # maximum of 80 characters\n",
    "            title=Column(String(80))\n",
    "            content=Column(String(20000))\n",
    "\n",
    "        self.Urllist=Urllist\n",
    "        self.table['urllist']=Urllist\n",
    "            \n",
    "        class Wordlist(Base):\n",
    "            __tablename__='wordlist'\n",
    "            word=Column(String(20), nullable=False)\n",
    "            id=Column(Integer, primary_key=True)\n",
    "        self.Wordlist=Wordlist\n",
    "        self.table['wordlist']=Wordlist\n",
    "            \n",
    "        class Wordlocation(Base):\n",
    "            __tablename__='wordlocation'\n",
    "            wordid=Column(Integer, ForeignKey(word.id), nullable=False)\n",
    "            urlid=Column(Integer, ForeignKey(url.id), nullable=False)\n",
    "            location=Column(Integer)\n",
    "            id=Column(Integer, primary_key=True)\n",
    "        self.Wordlocation=Wordlocation\n",
    "            \n",
    "        word=Relationship(Wordlist)\n",
    "        url=Relationship(Urllist)\n",
    "        \n",
    "        self.table['wordlocation']=Wordlocation\n",
    "            \n",
    "        class Link(Base):\n",
    "            __tablename__='link'\n",
    "            fromid=Column(Integer, ForeignKey(urlfrom.id), nullable=False)\n",
    "            toid=Column(Integer, ForeignKey(urlto.id), nullable=False)\n",
    "            id=Column(Integer, primary_key=True)\n",
    "            \n",
    "        urlfrom=Relationship(Urllist)\n",
    "        urlto=Relationship(Urllist)\n",
    "        \n",
    "        self.Link=Link\n",
    "        self.table['link']=Link\n",
    "            \n",
    "        class Linkword(Base):\n",
    "            __tablename='linkword'\n",
    "            linkid=Column(Integer, ForeignKey(link.id), nullable=False)\n",
    "            wordid=Column(Integer, ForeignKey(wordlist.id), nullable=False)\n",
    "            id=Column(Integer, primary_key=True)\n",
    "        self.Linkword=Linkword\n",
    "            \n",
    "        link=Relationship(Link)\n",
    "        \n",
    "        self.table['linkword']=Linkword\n",
    "        \n",
    "        Base.metadata.create_all(self.engine)\n",
    "                \n",
    "        \n",
    "    def isIndexed(self, url):\n",
    "        # return False\n",
    "        u = self.connection.execute \\\n",
    "        (\"select rowid from urlList where url='%s'\" % url).fetchone()\n",
    "        if u!=None:\n",
    "            #接下来检查它是否被wordLocation检索过了\n",
    "            v = self.connection.execute(\n",
    "            'select * from wordLocation where urlId=%d' % u[0]).fetchone()\n",
    "            if v!=None:\n",
    "                return True\n",
    "        return False \n",
    "    \n",
    "    # 从一个HTML网页中提取文字（不带html标签的）\n",
    "    def getTextOnly(self, soup):\n",
    "        # soup是BeautifulSoup object，包含了网页内容\n",
    "        # return None\n",
    "        v = soup.string\n",
    "        # v = soup.html.string\n",
    "        # 如果soup只有一个child，并且子子孙孙都只有一个child，v!=None, v就是最中心那个字符串\n",
    "        if v==None:\n",
    "        # the object soup is the html tag, it has many chidren, it contains more than one tag    \n",
    "            c=soup.contents\n",
    "            # A tag’s children are available in a list called .contents\n",
    "            resultText=''\n",
    "            for t in c:\n",
    "                subText = self.getTextOnly(t)\n",
    "                # 递归算法，更准确的说，是递归工具\n",
    "                resultText+=subText+'\\n'\n",
    "                # temp=subText+'\\n', resultText=resultText+temp\n",
    "                # 为了利于稍后某些度量的计算，在这一阶段保留各章节的前后顺序是很重要的\n",
    "            return resultText\n",
    "        else:\n",
    "            return v.strip()\n",
    "            # v is a string\n",
    "            # The method strip() returns a copy of the string \n",
    "            # in which all whitespace chars have been stripped from the beginning \n",
    "            # and the end of the string (default whitespace characters)\n",
    "            \n",
    "    # 根据空白字符进行分词处理，非字母或非数字的字符作为分隔符，还可以利用正则表达式来分词\n",
    "    def separateWords(self, text):\n",
    "        # return None\n",
    "        splitter = re.compile('\\\\W*')\n",
    "        # splitter是一个pattern\n",
    "        # \\W在计算机字符中有特定的意义？所以需要在前面使用转义字符\\\n",
    "        # 为了避免这么麻烦的转义，可以使用原始字符串，r'\\W*'即可。'\\t'是制表符，但r'\\t'只表示\\后跟着t\n",
    "        # \\w：用于匹配字母，数字或下划线字符；\\W：用于匹配所有与\\w不匹配的字符；*是0个以上\n",
    "        return [s.lower() for s in splitter.split(text) if s!='']\n",
    "        # 只要非字母、非数字、非下划线，都给它劈开，得到一个list，这个list最后可能有空字符串\n",
    "        # 转成小写字母时忽略空字符串\n",
    "        # 问题是C++这样的词无法得到c++，只能得到c，当然，可以使用更加复杂的正则表达式 \n",
    "        \n",
    "    # 辅助函数，用于获取数据库条目的id，如果条目不存在，就将其加入到数据库中\n",
    "    def getEntryId(self, table, field, value, createnew=True):\n",
    "        # rerurn None, for url and word, for two tables \n",
    "        # res = self.sesson.query(table.id).filter_by(setattr(table, field, value)).first()\n",
    "        # table is a class, field is a string\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    def getEntryId(self, table, temp_dict, field, value, createnew=True):  \n",
    "        res = self.sesson.query(table.id).filter_by(**temp_dict).first()\n",
    "        \n",
    "    '''\n",
    "        \n",
    "        # if res == None:\n",
    "        #     myrecord=table()\n",
    "        #     setattr(myrecord, field, value)\n",
    "        #     self.session.add(myrecord)\n",
    "        #     return self.sesson.query(table.id).filter_by(**temp_dict).first()\n",
    "        # else:\n",
    "        #     return res\n",
    "        \n",
    "        cur = self.connection.execute(\"select rowid from %s where %s = '%s'\" % (table, field, value))\n",
    "        # table, field and value are all strings, value will be used as a string\n",
    "        # 得到的cur是个iterator, 拥有fetchone()这个method\n",
    "        # This is insecure, you can do the following instead\n",
    "        # t = (table, field, value)\n",
    "        # cur = self.con.execute(\"select rowid from ? where ? = '?'\" , t)\n",
    "        res = cur.fetchone()\n",
    "        # res是一个只包含一个元素的list，或者是None\n",
    "        if res == None:\n",
    "            cur = self.connection.execute(\"insert into %s(%s) values ('%s')\" (table, field, value))\n",
    "            return cur.lastrowid\n",
    "        else:\n",
    "            return res[0]        \n",
    "        \n",
    "    # 为每个网页建立索引\n",
    "    def addToIndex(self, url, soup): \n",
    "        # for one table\n",
    "        # print 'Indexing %s' % url\n",
    "        if self.isIndexed(url): \n",
    "            return\n",
    "        print 'Indexing'+url\n",
    "        \n",
    "        # 得到URL的id\n",
    "        # urlId = self.getEntryId(self.Urllist, 'url', url)\n",
    "        urlId = self.getEntryId('urllist', 'url', url)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        temp_dict={'self.Urllist.url':url}\n",
    "        urlId = self.getEntryId(self.Urllist, temp_dict, 'url', url)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        thearticle=self.sesson.query(self.Urllist).filter_by(id=urlId).first()\n",
    "        thearticle.content=text\n",
    "        self.session.add(thearticle)\n",
    "        \n",
    "        # 获取每个单词\n",
    "        text = self.getTextOnly(soup)\n",
    "        \n",
    "        words = self.seperateWords(text)\n",
    "        # 分词可以使用regular expression\n",
    "        # 此处也可以使用中文分词程序\n",
    "        # words is a list        \n",
    "        \n",
    "        # 将每个单词与该url关联\n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "            if word in ignorewords:\n",
    "                continue\n",
    "            wordId = self.getEntryId('wordlist','word', word)\n",
    "            wordlocatoin_i=self.Wordlocation(wordid=wordId, urlid=urlId, location=i)\n",
    "            self.session.add(wordlocatoin_i)\n",
    "            \n",
    "        # self.session.commit()\n",
    "        \n",
    "    # 添加一个关联两个网页的链接，本质上是把这个链接和相关文字计入数据库，为以后的数据处理做准备\n",
    "    def addLinkRef(self, urlFrom, urlTo, linkText):\n",
    "        # for two tables\n",
    "        # pass\n",
    "        # linkfrom=self.sesson.query(self.Urllist.id).filter_by(url=urlFrom).first()\n",
    "        linkfrom=self.getEntryId('urllist','url', urlFrom)\n",
    "        # linkto=self.sesson.query(self.Urllist.id).filter_by(url=urlTo).first()\n",
    "        linkto=self.getEntryId('urllist','url', urlTo)\n",
    "        thelink=self.Link(fromid=linkfrom, toid=linkto)\n",
    "        self.session.add(thelink)\n",
    "        # theid=self.sesson.query(self.Link.id).filter_by(fromid=urlFrom, toid=urlTo).first()\n",
    "        \n",
    "        words = self.seperateWords(linkText)\n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "            wordId = self.getEntryId('wordlist','word', word)\n",
    "        \n",
    "            thelinktext_i=self.Linkword(link=thelink, wordid=wordId)\n",
    "            self.session.add(thelinktext_i)\n",
    "        \n",
    "    def crawl(self, pages, depth=2):\n",
    "        # pass\n",
    "        for i in range(depth):\n",
    "            newPages = set()\n",
    "            for page in pages:\n",
    "                try:\n",
    "                    c=urllib2.urlopen(page)\n",
    "                except:\n",
    "                    print \"Could not open %s\" % page\n",
    "                    continue\n",
    "                soup = BeautifulSoup(c.read())\n",
    "                self.addToIndex(page, soup)\n",
    "                # addToIndex\n",
    "                \n",
    "                # 以下是把page中链接的网页全部用爬虫爬一遍\n",
    "                links = soup('a')\n",
    "                # links = soup.find_all('a')\n",
    "                # <a> 标签定义超链接，用于从一张页面链接到另一张页面。\n",
    "                # <a> 元素最重要的属性是 href 属性，它指示链接的目标。\n",
    "                for link in links:\n",
    "                    if ('href' in dict(link.attrs)):\n",
    "                        # 该链接有href属性的话\n",
    "                        url = urljoin(page, link['href'])\n",
    "                        # link is a dict\n",
    "                        # urljoin https://docs.python.org/2/library/urlparse.html\n",
    "                        if url.find(\"'\")!=-1: \n",
    "                            continue\n",
    "                            # 网址中出现'，是不合格的网址，进入新的链接\n",
    "                        url = url.split('#')[0]\n",
    "                        # 去掉链接字符串标明位置的部分\n",
    "                        if url[0:4] = 'http' and not self.isIndexed(url):\n",
    "                            newPages.add(url)\n",
    "                        linkText = self.getTextOnly(link)\n",
    "                        # link is an object in soup class\n",
    "                        # links = soup('a'), for link in links\n",
    "                        self.addLinkRef(page, url, linkText)\n",
    "                        \n",
    "                self.dbcommit()\n",
    "            pages = newpages\n",
    "            # 更新global变量pages，可以不断调用crawl来进行增加搜索深度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# searchEngine3.py\n",
    "\n",
    "# class Searcher:\n",
    "class Classifier:\n",
    "    def __init__(self, dbname):\n",
    "        '''\n",
    "        构造函数\n",
    "        '''\n",
    "        self.engine = create_engine('mysql+mysqlconnector://new_user:new_password@localhost:3306/%s'% dbname, \n",
    "                        echo=False)        \n",
    "        DBSession = sessionmaker(bind=self.engine)\n",
    "        self.session=DBSession()\n",
    "        # self.con = self.engine.connect()\n",
    "        self.con = self.session\n",
    "\n",
    "    def getmatchrows(self, q):\n",
    "        \n",
    "        # 构造用于sql查询的语句字符串\n",
    "        fieldlist = 'w0.urlId' # fieldlist是字段，先给出来的这一部分是复杂的查询语句的一部分\n",
    "        talbelist = '' # 表\n",
    "        clauselist = '' # 查询的条件\n",
    "        wordids = []\n",
    "        \n",
    "        # 把输入的字符串根据空格拆分单词\n",
    "        words = q.split(' ')\n",
    "        tablenumber = 0\n",
    "        \n",
    "        for word in words:\n",
    "            # 获取单词的ID\n",
    "            wordrow = self.con.execute( \\\n",
    "                    \"select rowid frome wordList where word='%s'\" % word).fetchone()\n",
    "            if wordrow!=None:\n",
    "                wordid = wordrow[0]\n",
    "                wordids.append(wordid)\n",
    "                if tablenumber>0:\n",
    "                    tablelist+=','\n",
    "                    clauselist+=' and '\n",
    "                    clauselist+='w%d.urlId=w%d.urlId' % (tablenumber-1, tablenumber)\n",
    "                fieldlist+=',w%d.location' % tablenumber\n",
    "                tablelist+='wordLoaction w%d' % tablenumber\n",
    "                # wordLocation w0 和 wordLocation w1都是wordLocation这个数据表的引用或者别名\n",
    "                # 所谓引用，就是像值传递一样去使用，但本质上做的事指针的事情\n",
    "                clauselist+='w%d.wordId=%d' % (tablenumber, wordId)\n",
    "                tablenumber+=1\n",
    "                # 以上构造了复杂的sql语句，比如\n",
    "                # select w0.urlId, w0.location, w1.location\n",
    "                # from wordLocation w0, wordLocation w1\n",
    "                # where w0.wordId=10\n",
    "                # and w0.urlId=w1.urlId\n",
    "                # and w1.wordId=17\n",
    "                \n",
    "        fullquery='select %s from %s where %s' % (fieldlist, tablelist, clauselist)\n",
    "        cur=self.con.execute(fullquery)\n",
    "        # an iterator object\n",
    "        rows=[row for row in cur]\n",
    "        # from an iterator to a list\n",
    "                    \n",
    "        return rows, wordids\n",
    "        # 文章的信息，在文章中的位置的信息，词的信息\n",
    "        # , means a tuple should be constructed with two items rows and wordids\n",
    "        # rows is a list and wordids is a list, therefore, the tuple contains two lists         \n",
    "\n",
    "    def normalizescores(self, scores, smallIsBetter=0):\n",
    "        # 归一化处理函数；评价值介于0和1之间，越大表示越好\n",
    "        vsmall = 0.00001 # 避免被零整除\n",
    "        if smallIsBetter:\n",
    "            minscore=min(scores.values())\n",
    "            return dict([(u, float(minscore)/max(vsmall, L)) for (u, L) in scores.items()])\n",
    "            # L很小或者是0的时候，就使用vsmall做分母\n",
    "        else:\n",
    "            maxscore=max(scores.values())\n",
    "            if maxscore==0:\n",
    "                maxscore=vsmall\n",
    "            return dict([(u, float(c)/maxscore) for (u,c) in scores.items()])        \n",
    "        \n",
    "    def frequencyscore(self, rows):\n",
    "        counts=dict([(row[0],0) for row in rows])\n",
    "        # rows is a list of list, row[0] is urlID\n",
    "        for row in rows:\n",
    "            # len(rows)表明了各个url被搜到的相对的次数，对应着词频，如果是多个词的搜索，更复杂些\n",
    "            counts[row[0]]+=1\n",
    "        return self.normalizescores(counts)\n",
    "        # counts显然是越大越好，此处做归一化处理        \n",
    "        \n",
    "    def getscoredlist(self, rows, wordids):\n",
    "        totalscores=dict([(row[0], 0) for row in rows])\n",
    "        # row[0]是urlId\n",
    "        \n",
    "        # 此处是稍后放置评价函数的地方\n",
    "        # weights=[]\n",
    "        weights=[(1.0, self.frequencyscore(rows))]\n",
    "        # self.frequencyscore(rows) is a dict with urls as the keys\n",
    "        # 此处的weights只有一个元素(1.0, self.frequencyscore(rows))，是很简单的一种形式\n",
    "        # 在这种情况下，底下的for循环只循环一次\n",
    "        # 实际上，可能是有多个元素的更复杂的形式，也就是说，不是单一的评价方式，是混合评价方式\n",
    "        # 每种方式都有各自的权重\n",
    "        \n",
    "        # weights=[(1.0, self.locationscore(rows))]\n",
    "        # weights=[(0.7, self.frequencyscore(rows)), (0.3, self.locationscore(rows))]\n",
    "\n",
    "        # weights=[(1.0, self.frequencyscore(rows)),\n",
    "        #          (1.5, self.locationscore(rows))]\n",
    "\n",
    "        # weights=[(2, self.frequencyscore(rows)),\n",
    "        #          (1, self.locationscore(rows)),\n",
    "        #          (1, self.distancescore(rows))]\n",
    "        \n",
    "        # 修改weights列表，将PageRank算法纳入其中\n",
    "        # 最终的搜索结果会综合考虑网页内容和PageRank排名的影响\n",
    "        # 对于返回更好层次和更大众化的网页而言，PageRank无疑是一种有效的度量方法\n",
    "        # weights=[(1.0, self.frequencyscore(rows)),\n",
    "        #          (1.0, self.locationscore(rows)),\n",
    "        #          (1.0, self.distancescore(rows))，\n",
    "        #          (1.0, self.pagerankscore(rows))]\n",
    "        \n",
    "        # 将链接文本考虑进去\n",
    "        # 对于下面的一系列度量，并不存在一组标准的权重分配，能够适应所有的情况\n",
    "        # 我们所用到的度量方法，以及赋予它们的权重系数，很大程度上取决于我们正在试图构建的应用\n",
    "        # weights=[(1.0, self.frequencyscore(rows)),\n",
    "        #          (1.0, self.locationscore(rows)),\n",
    "        #          (1.0, self.distancescore(rows))，\n",
    "        #          (1.0, self.pagerankscore(rows)),\n",
    "        #          (1.0, self.linktextscore(rows, wordids))]        \n",
    "        \n",
    "        for (weight, scores) in weights:\n",
    "            # weight is a float number, scores is a dict\n",
    "            for url in totalscores:\n",
    "                totalscores[url]+=weight*scores[url]\n",
    "                \n",
    "        return totalscores        \n",
    "        \n",
    "    def query(self, q):\n",
    "        rows, wordids=self.getmatchrows(q)\n",
    "        scores=self.getscoredlist(rows, wordids)\n",
    "        rankedscores=sorted([(score, url) for (url, score) in scores.items()], reverse=1)\n",
    "        # sorted一个二元元组组成的list，是按元组中第一个元素来排序的; 先按第一个元素，再按第二个元素排序？\n",
    "        # timsort, an adaptive mergesort\n",
    "        for (score, urlid) in rankedscores[0:10]:\n",
    "            print '%f\\t%s' % (score, self.geturlname(urlid))\n",
    "            # %f是小数格式，%d是整数格式，%s是字符格式\n",
    "        return rankedscores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for unit test of searchEngine3.py\n",
    "# Unit test by the python library unittest\n",
    "\n",
    "import unittest\n",
    "import os\n",
    "\n",
    "# datadir=os.path.join(os.path.expanduser('~'),'Documents','workspace','data')\n",
    "    \n",
    "class TestMyClass(unittest.TestCase):\n",
    "    \n",
    "    # hand1=os.path.join(datadir, mydata['file1'])\n",
    "    # hand2=os.path.join(datadir, mydata['file2'])\n",
    "    # AttributeError: 'dict' object has no attribute 'file2'\n",
    "    \n",
    "    def test_sum1(self):\n",
    "        self.assertEqual(Searcher('menagerie').getmatchrows('sequencing ngs'), \\ \n",
    "                         (therows, thewordids))\n",
    "        # (therows, thewordids) are concrete tuples\n",
    "        \n",
    "    def test_sum2(self):\n",
    "        self.assertEqual(Searcher('menagerie').query('sequencing ngs'), thelist)  \n",
    "        # thelist is the specific right list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unit test of searchEngine3.py\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestMyClass)\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mediaWeb.py\n",
    "\n",
    "from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer\n",
    "# import cgi\n",
    "\n",
    "import searchEngine2\n",
    "crawler = searchEngine2.Crawler('menagerie')\n",
    "crawler.createindextables()\n",
    "pages= \\\n",
    "['http://www.bio360.net', 'http://www.bioon.com']\n",
    "crawler.crawl(pages)\n",
    "\n",
    "import searchEngine3\n",
    "# e=searchEngine.Searcher('menagerie')\n",
    "e=searchEngine.Classifier('menagerie')\n",
    "rankedsores=e.query('sequencing ngs')\n",
    "top10=rankedscores[0:10]\n",
    "\n",
    "session = crawler.session\n",
    "\n",
    "class webServerHandler(BaseHTTPRequestHandler):\n",
    "    # the http handler calss\n",
    "\n",
    "    def do_GET(self):\n",
    "        try:\n",
    "            if self.path.endswith(\"/articles\"):\n",
    "                articles = [session.query(Urllist).filter_by(url=theurl).all() for \\\n",
    "                theurl in top10]\n",
    "# http://stackoverflow.com/questions/29326297/sqlalchemy-filter-by-field-in-list-but-keep-original-order\n",
    "                output = \"\"\n",
    "                self.send_response(200)\n",
    "                self.send_header('Content-type', 'text/html')\n",
    "                self.end_headers()\n",
    "                output += \"<html><body>\"\n",
    "                output += \"<h1>Sequencing</h1><br><br><br>\"\n",
    "                for article in articles:\n",
    "                    output += restaurant.content\n",
    "                    output += \"</br></br></br>\"\n",
    "                    # 三次换行\n",
    "                    # The HTML <br> element produces a line break in text (carriage-return). \n",
    "                    # It is useful for writing a poem or an address, \n",
    "                    # where the division of lines is significant.\n",
    "\n",
    "                output += \"</body></html>\"\n",
    "                self.wfile.write(output)\n",
    "                return\n",
    "        except IOError:\n",
    "            self.send_error(404, 'File Not Found: %s' % self.path)\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        server = HTTPServer(('', 8080), webServerHandler)\n",
    "        # http server instance\n",
    "        print 'Web server running...open localhost:8080/restaurants in your browser'\n",
    "        server.serve_forever()\n",
    "    except KeyboardInterrupt:\n",
    "        print '^C received, shutting down server'\n",
    "        server.socket.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
