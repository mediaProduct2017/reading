{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl with scrapy for news in biomedical news websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# crawlerClass.py\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import relationship, sessionmaker\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self, dbname):\n",
    "        self.engine = create_engine('mysql+mysqlconnector://new_user:new_password@localhost:3306/%s'% dbname, \n",
    "                        echo=False)        \n",
    "        DBSession = sessionmaker(bind=self.engine)\n",
    "        self.session=DBSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shell\n",
    "\n",
    "sudo /usr/local/mysql/support-files/mysql.server start\n",
    "mysql -u root -p\n",
    "\n",
    "mysql> CREATE DATABASE newsdb;\n",
    "Query OK, 1 row affected (0.01 sec)\n",
    "\n",
    "mysql> USE newsdb;\n",
    "Database changed\n",
    "\n",
    "mysql> GRANT ALL ON newsdb.* TO 'new_user'@'localhost';\n",
    "\n",
    "mysql> FLUSH PRIVILEGES;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def test_del(self):\n",
    "        c = 'newsdb'\n",
    "        try:\n",
    "            a = Crawler(c)\n",
    "            print a.session\n",
    "            # a.session.close()\n",
    "            # a.__del__()\n",
    "        except ValueError:\n",
    "            self.fail('Crawler del raised Exception unexpectedly')\n",
    "\n",
    "\n",
    "    def test_dbcommit(self):\n",
    "        c = 'newsdb'\n",
    "        try:\n",
    "            Crawler(c).dbcommit()\n",
    "        except ValueError:\n",
    "            self.fail('Crawler commit raised Exception unexpectedly')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def __del__(self):\n",
    "        # pass\n",
    "        # self.connection.close()\n",
    "        self.session.close() \n",
    "    \n",
    "    # 爬到的数据放入数据库\n",
    "    def dbcommit(self):\n",
    "        # pass\n",
    "        self.session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, ForeignKey, Integer, String\n",
    "from sqlalchemy.ext.declarative import declarative_base    \n",
    "    \n",
    "    def createindextables(self):\n",
    "    # self means it is instance method, not class method or static method\n",
    "        self.table=dict()\n",
    "        Base=declarative_base()\n",
    "\n",
    "        class Urllist(Base):\n",
    "            __tablename__='urllist'\n",
    "            \n",
    "            id=Column(Integer, primary_key=True, autoincrement=True)\n",
    "            # autoincrement=True by default\n",
    "            \n",
    "            url=Column(String(80), nullable=False)\n",
    "            # maximum of 80 characters\n",
    "            title=Column(String(80))\n",
    "            content=Column(String(20000))\n",
    "\n",
    "        self.table['urllist']=Urllist\n",
    "            \n",
    "        class Wordlist(Base):\n",
    "            __tablename__='wordlist'\n",
    "            word=Column(String(20), nullable=False)\n",
    "            id=Column(Integer, primary_key=True)\n",
    "\n",
    "        self.table['wordlist']=Wordlist\n",
    "            \n",
    "        class Wordlocation(Base):\n",
    "            __tablename__='wordlocation'\n",
    "            wordid=Column(Integer, ForeignKey(word.id), nullable=False)\n",
    "            urlid=Column(Integer, ForeignKey(url.id), nullable=False)\n",
    "            location=Column(Integer)\n",
    "            id=Column(Integer, primary_key=True)\n",
    "            \n",
    "        word=Relationship(Wordlist)\n",
    "        url=Relationship(Urllist)\n",
    "        \n",
    "        self.table['wordlocation']=Wordlocation\n",
    "            \n",
    "        class Link(Base):\n",
    "            __tablename__='link'\n",
    "            fromid=Column(Integer, ForeignKey(urlfrom.id), nullable=False)\n",
    "            toid=Column(Integer, ForeignKey(urlto.id), nullable=False)\n",
    "            id=Column(Integer, primary_key=True)\n",
    "            \n",
    "        urlfrom=Relationship(Urllist)\n",
    "        urlto=Relationship(Urllist)\n",
    "        \n",
    "        self.table['link']=Link\n",
    "            \n",
    "        class Linkword(Base):\n",
    "            __tablename='linkword'\n",
    "            linkid=Column(Integer, ForeignKey(link.id), nullable=False)\n",
    "            wordid=Column(Integer, ForeignKey(wordlist.id), nullable=False)\n",
    "            id=Column(Integer, primary_key=True)\n",
    "            \n",
    "        link=Relationship(Link)\n",
    "        \n",
    "        self.table['linkword']=Linkword\n",
    "        \n",
    "        Base.metadata.create_all(self.engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# crawlerClass.py\n",
    "\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "import sys\n",
    "# sys.stdout may be changed. When echo parameter in create_engine is True, \n",
    "# the engine will log ot stdout. At that circumstance, sys module will be used.\n",
    "\n",
    "from sqlalchemy import Column, ForeignKey, Integer, String\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "# ext means extension, declarative 说明，declarative_base用来生成和数据表相关的类\n",
    "from sqlalchemy.orm import relationship, sessionmaker\n",
    "# orm is Object-relational mapping, which links object with database\n",
    "\n",
    "import urllib2\n",
    "# there is even urllib5, but now most people use requests to replace urllib series\n",
    "from BeautifulSoup import *\n",
    "# there is beautifulsoup4, which is updated in 2017\n",
    "from urlparse import urljoin\n",
    "# there is urlparse3 (2016) and urlparse4 (2016)\n",
    "import re\n",
    "# regular expression\n",
    "\n",
    "ignoreWords = set(['the', 'of', 'to', 'and', 'a', 'in', 'is', 'it'])\n",
    "\n",
    "class Crawler:\n",
    "    # 从一小组网页开始进行广度优先搜索，直到某一给定深度，期间为所有网页建立索引\n",
    "    # 一般都是静态网址\n",
    "    def __init__(self, dbname):\n",
    "        # self.engine = create_engine('mysql+mysqlconnector://new_user:new_password@localhost:3306/menagerie', \n",
    "        #                echo=False)\n",
    "        self.engine = create_engine('mysql+mysqlconnector://new_user:new_password@localhost:3306/%s'% dbname, \n",
    "                        echo=False)        \n",
    "        DBSession = sessionmaker(bind=self.engine)\n",
    "        # DBSession is a class?\n",
    "        self.session=DBSession()\n",
    "        # self.connection = self.engine.connect()\n",
    "        self.connection = self.session\n",
    "        # self.session.execute()\n",
    "\n",
    "    # 解构函数\n",
    "    def __del__(self):\n",
    "        # pass\n",
    "        # self.connection.close()\n",
    "        self.session.close() \n",
    "    \n",
    "    # 爬到的数据放入数据库\n",
    "    def dbcommit(self):\n",
    "        # pass\n",
    "        self.session.commit()\n",
    "\n",
    "    def createindextables(self):\n",
    "    # 外部调用接口，public\n",
    "    # self means it is instance method, not class method\n",
    "        self.table=dict()\n",
    "        Base=declarative_base()\n",
    "# declarative_base will let sqlalchemy know the classes are special sqlalchemy classes\n",
    "# which correspond to tables in our database\n",
    "        class Urllist(Base):\n",
    "            __tablename__='urllist'\n",
    "            \n",
    "            id=Column(Integer, primary_key=True, autoincrement=True)\n",
    "            # autoincrement=True by default\n",
    "            \n",
    "            url=Column(String(80), nullable=False)\n",
    "            # maximum of 80 characters\n",
    "            title=Column(String(80))\n",
    "            content=Column(String(20000))\n",
    "\n",
    "        self.Urllist=Urllist\n",
    "        self.table['urllist']=Urllist\n",
    "            \n",
    "        class Wordlist(Base):\n",
    "            __tablename__='wordlist'\n",
    "            word=Column(String(20), nullable=False)\n",
    "            id=Column(Integer, primary_key=True)\n",
    "        self.Wordlist=Wordlist\n",
    "        self.table['wordlist']=Wordlist\n",
    "            \n",
    "        class Wordlocation(Base):\n",
    "            __tablename__='wordlocation'\n",
    "            wordid=Column(Integer, ForeignKey(word.id), nullable=False)\n",
    "            urlid=Column(Integer, ForeignKey(url.id), nullable=False)\n",
    "            location=Column(Integer)\n",
    "            id=Column(Integer, primary_key=True)\n",
    "        self.Wordlocation=Wordlocation\n",
    "            \n",
    "        word=Relationship(Wordlist)\n",
    "        url=Relationship(Urllist)\n",
    "        \n",
    "        self.table['wordlocation']=Wordlocation\n",
    "            \n",
    "        class Link(Base):\n",
    "            __tablename__='link'\n",
    "            fromid=Column(Integer, ForeignKey(urlfrom.id), nullable=False)\n",
    "            toid=Column(Integer, ForeignKey(urlto.id), nullable=False)\n",
    "            id=Column(Integer, primary_key=True)\n",
    "            \n",
    "        urlfrom=Relationship(Urllist)\n",
    "        urlto=Relationship(Urllist)\n",
    "        \n",
    "        self.Link=Link\n",
    "        self.table['link']=Link\n",
    "            \n",
    "        class Linkword(Base):\n",
    "            __tablename='linkword'\n",
    "            linkid=Column(Integer, ForeignKey(link.id), nullable=False)\n",
    "            wordid=Column(Integer, ForeignKey(wordlist.id), nullable=False)\n",
    "            id=Column(Integer, primary_key=True)\n",
    "        self.Linkword=Linkword\n",
    "            \n",
    "        link=Relationship(Link)\n",
    "        \n",
    "        self.table['linkword']=Linkword\n",
    "        \n",
    "        Base.metadata.create_all(self.engine)\n",
    "                \n",
    "        \n",
    "    def isIndexed(self, url):\n",
    "        # return False\n",
    "        u = self.connection.execute \\\n",
    "        (\"select rowid from urlList where url='%s'\" % url).fetchone()\n",
    "        # 检查是否被urlList检索过\n",
    "        if u!=None:\n",
    "            #接下来检查它是否被wordLocation检索过了\n",
    "            v = self.connection.execute(\n",
    "            'select * from wordLocation where urlId=%d' % u[0]).fetchone()\n",
    "            if v!=None:\n",
    "                return True\n",
    "        return False \n",
    "    \n",
    "    # 从一个HTML网页中提取文字（不带html标签的），也有可能是网页的一部分文字内容\n",
    "    def getTextOnly(self, soup):\n",
    "        # soup是BeautifulSoup object，包含了网页内容\n",
    "        # return None\n",
    "        v = soup.string\n",
    "        # v = soup.html.string\n",
    "        # 如果soup只有一个child，并且子子孙孙都只有一个child，v!=None, v就是最中心那个字符串\n",
    "        if v==None:\n",
    "        # the object soup is the html tag, it has many chidren, it contains more than one tag    \n",
    "            c=soup.contents\n",
    "            # A tag’s children are available in a list called .contents\n",
    "            resultText=''\n",
    "            for t in c:\n",
    "                subText = self.getTextOnly(t)\n",
    "                # 递归算法，更准确的说，是递归工具\n",
    "                resultText+=subText+'\\n'\n",
    "                # temp=subText+'\\n', resultText=resultText+temp\n",
    "                # 为了利于稍后某些度量的计算，在这一阶段保留各章节的前后顺序是很重要的\n",
    "            return resultText\n",
    "        else:\n",
    "            return v.strip()\n",
    "            # v is a string\n",
    "            # The method strip() returns a copy of the string \n",
    "            # in which all whitespace chars have been stripped from the beginning \n",
    "            # and the end of the string (default whitespace characters)\n",
    "            \n",
    "    # 根据空白字符进行分词处理，非字母或非数字的字符作为分隔符，还可以利用正则表达式来分词\n",
    "    # 中文的话，可以换成jieba分词\n",
    "    def separateWords(self, text):\n",
    "        # return None\n",
    "        splitter = re.compile('\\\\W*')\n",
    "        # splitter是一个pattern\n",
    "        # \\W在计算机字符中有特定的意义？所以需要在前面使用转义字符\\\n",
    "        # 为了避免这么麻烦的转义，可以使用原始字符串，r'\\W*'即可。'\\t'是制表符，但r'\\t'只表示\\后跟着t\n",
    "        # \\w：用于匹配字母，数字或下划线字符；\\W：用于匹配所有与\\w不匹配的字符；*是0个以上\n",
    "        # hyphen连字号两端会被当成两个单词处理\n",
    "        return [s.lower() for s in splitter.split(text) if s!='']\n",
    "        # 只要非字母、非数字、非下划线，都给它劈开，得到一个list，这个list最后可能有空字符串\n",
    "        # 转成小写字母时忽略空字符串\n",
    "        # 问题是C++这样的词无法得到c++，只能得到c，当然，可以使用更加复杂的正则表达式 \n",
    "        \n",
    "    # 辅助函数，用于获取数据库条目的id，如果条目不存在，就将其加入到数据库中\n",
    "    # 可能作用于urllist和wordlist这两个数据表\n",
    "    def getEntryId(self, table, field, value, createnew=True):\n",
    "        # rerurn None, for url and word, for two tables \n",
    "        # res = self.sesson.query(table.id).filter_by(setattr(table, field, value)).first()\n",
    "        # table is a class, field is a string\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    def getEntryId(self, table, temp_dict, field, value, createnew=True):  \n",
    "        res = self.sesson.query(table.id).filter_by(**temp_dict).first()\n",
    "        \n",
    "    '''\n",
    "        \n",
    "        # if res == None:\n",
    "        #     myrecord=table()\n",
    "        #     setattr(myrecord, field, value)\n",
    "        #     self.session.add(myrecord)\n",
    "        #     return self.sesson.query(table.id).filter_by(**temp_dict).first()\n",
    "        # else:\n",
    "        #     return res\n",
    "        \n",
    "        cur = self.connection.execute(\"select rowid from %s where %s = '%s'\" % (table, field, value))\n",
    "        # table, field and value are all strings, value will be used as a string\n",
    "        # 得到的cur是个iterator, 拥有fetchone()这个method\n",
    "        # This is insecure, you can do the following instead\n",
    "        # t = (table, field, value)\n",
    "        # cur = self.con.execute(\"select rowid from ? where ? = '?'\" , t)\n",
    "        res = cur.fetchone()\n",
    "        # res是一个只包含一个元素的list，或者是None\n",
    "        if res == None:\n",
    "            cur = self.connection.execute(\"insert into %s(%s) values ('%s')\" (table, field, value))\n",
    "            return cur.lastrowid\n",
    "        else:\n",
    "            return res[0]        \n",
    "        \n",
    "    # 为每个网页建立索引，具体完成将文章内容放入数据库，以及文章与词汇的关联\n",
    "    def addToIndex(self, url, soup): \n",
    "        # for one table\n",
    "        # print 'Indexing %s' % url\n",
    "        if self.isIndexed(url): \n",
    "            return\n",
    "        print 'Indexing'+url\n",
    "        \n",
    "        # 得到URL的id\n",
    "        # urlId = self.getEntryId(self.Urllist, 'url', url)\n",
    "        urlId = self.getEntryId('urllist', 'url', url)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        temp_dict={'self.Urllist.url':url}\n",
    "        urlId = self.getEntryId(self.Urllist, temp_dict, 'url', url)\n",
    "        \n",
    "        '''\n",
    "        # 获取网页中的文章内容\n",
    "        text = self.getTextOnly(soup)\n",
    "        \n",
    "        thearticle=self.sesson.query(self.Urllist).filter_by(id=urlId).first()\n",
    "        thearticle.content=text\n",
    "        self.session.add(thearticle)\n",
    "        \n",
    "        # 获取每个单词\n",
    "        words = self.seperateWords(text)\n",
    "        # 分词可以使用regular expression\n",
    "        # 此处也可以使用中文分词程序\n",
    "        # words is a list        \n",
    "        \n",
    "        # 将每个单词与该url关联\n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "            if word in ignorewords:\n",
    "                continue\n",
    "            wordId = self.getEntryId('wordlist','word', word)\n",
    "            wordlocatoin_i=self.Wordlocation(wordid=wordId, urlid=urlId, location=i)\n",
    "            self.session.add(wordlocatoin_i)\n",
    "            \n",
    "        # self.session.commit()\n",
    "        \n",
    "    # 添加一个关联两个网页的链接，本质上是把这个链接和相关文字计入数据库，为以后的数据处理做准备\n",
    "    def addLinkRef(self, urlFrom, urlTo, linkText):\n",
    "        # for two tables\n",
    "        # pass\n",
    "        # linkfrom=self.sesson.query(self.Urllist.id).filter_by(url=urlFrom).first()\n",
    "        linkfrom=self.getEntryId('urllist','url', urlFrom)\n",
    "        # linkto=self.sesson.query(self.Urllist.id).filter_by(url=urlTo).first()\n",
    "        linkto=self.getEntryId('urllist','url', urlTo)\n",
    "        thelink=self.Link(fromid=linkfrom, toid=linkto)\n",
    "        self.session.add(thelink)\n",
    "        # theid=self.sesson.query(self.Link.id).filter_by(fromid=urlFrom, toid=urlTo).first()\n",
    "        \n",
    "        words = self.seperateWords(linkText)\n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "            wordId = self.getEntryId('wordlist','word', word)\n",
    "        \n",
    "            thelinktext_i=self.Linkword(link=thelink, wordid=wordId)\n",
    "            self.session.add(thelinktext_i)\n",
    "        \n",
    "    def crawl(self, pages, depth=2):\n",
    "        # 外部调用接口，public\n",
    "        # 可以反复调用，深度不断加深\n",
    "        # pass\n",
    "        for i in range(depth):\n",
    "            newPages = set()\n",
    "            for page in pages:\n",
    "                try:\n",
    "                    c=urllib2.urlopen(page)\n",
    "                except:\n",
    "                    print \"Could not open %s\" % page\n",
    "                    continue\n",
    "                soup = BeautifulSoup(c.read())\n",
    "                self.addToIndex(page, soup)\n",
    "                # addToIndex\n",
    "                # 把文章、词汇、关联分别放入三个数据库\n",
    "                \n",
    "                # 以下是把page中链接的网页全部用爬虫爬一遍\n",
    "                links = soup('a')\n",
    "                # links = soup.find_all('a')\n",
    "                # <a> 标签定义超链接，用于从一张页面链接到另一张页面。\n",
    "                # <a> 元素最重要的属性是 href 属性，它指示链接的目标。\n",
    "                for link in links:\n",
    "                    if ('href' in dict(link.attrs)):\n",
    "                        # 该链接有href属性的话\n",
    "                        url = urljoin(page, link['href'])\n",
    "                        # link is a dict\n",
    "                        # urljoin https://docs.python.org/2/library/urlparse.html\n",
    "                        if url.find(\"'\")!=-1: \n",
    "                            continue\n",
    "                            # 网址中出现'，是不合格的网址，进入新的链接\n",
    "                        url = url.split('#')[0]\n",
    "                        # 去掉链接字符串标明位置的部分\n",
    "                        if url[0:4] = 'http' and not self.isIndexed(url):\n",
    "                            newPages.add(url)\n",
    "                        linkText = self.getTextOnly(link)\n",
    "                        # link is an object in soup class\n",
    "                        # links = soup('a'), for link in links\n",
    "                        self.addLinkRef(page, url, linkText)\n",
    "                        # 将链接和链接文字分别放入两个数据库\n",
    "                        \n",
    "                self.dbcommit()\n",
    "            pages = newpages\n",
    "            # 更新global变量pages，可以不断调用crawl来进行增加搜索深度\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
